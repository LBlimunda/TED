<!DOCTYPE html>
<html lang="pt">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Teorema da Entropia Democrática</title>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <style>
        body { font-family: Arial, sans-serif; margin: 20px; background-color: #f4f4f4; color: #333; }
        .container { max-width: 800px; margin: auto; min-height:100%;background: white; padding: 20px; border-radius: 8px; box-shadow: 0 0 10px rgba(0,0,0,0.1); }
        h1 { padding-top:10px; text-align: center; color: #2c3e50; }
        p,h2,h3{padding-top:10px; display:block}
        h2 { color: #34495e; }
        h3 { color: #34495e; }
        .highlight{ background-color:lime; min-height:1em}
        .formula { background: #ecf0f1; padding: 10px; border-left: 5px solid #3498db; margin: 10px 0; font-family: monospace; }
        .simulation { margin: 20px 0; }
        input { padding: 8px; margin: 5px; }
        canvas { max-width: 100%; height: 300px; }
        .license { text-align: center; margin-top: 20px; font-size: 0.9em; color: #666; }
        .license-initial { text-align: center; margin-bottom: 20px; font-size: color: #666; }
        .ted-ia-section { margin-top: 2rem; }
    .ted-ia-title { margin-bottom: 1rem; }
    .ted-ia-subtitle { margin-top: 1.5rem; margin-bottom: 0.5rem; }
    .ted-ia-paragraph { margin: 0.5rem 0; }
    .ted-ia-formula { background: #f5f7fa; border-left: 5px solid #3498db; padding: 10px; font-family: monospace; }
    .ted-ia-highlight { background: #fff7e6; border-left: 5px solid #f39c12; padding: 10px; }
    .ted-ia-table { width: 100%; border-collapse: collapse; margin-top: 0.75rem; }
    .ted-ia-table th, .ted-ia-table td { border: 1px solid #ccc; padding: 8px; vertical-align: top; }
    .ted-ia-table th { background: #eef2f6; text-align: left; }
    .ted-ia-note { font-size: 0.9em; margin-top: 1rem; }
    .fmu-section { margin-top: 1.5rem; }
    .fmu-title { color:#34495e; margin-bottom: .5rem; }
    .fmu-par { margin:.5rem 0; text-align: justify; }
    .fmu-formula { background:#f5f7fa; border-left:4px solid #3498db; padding:10px; font-family:monospace; }
    .fmu-list { margin:.5rem 0 .25rem 1rem; }
    .fmu-list li { margin:.25rem 0; }
 
    @media (max-width: 600px) { .container { padding: 10px; } canvas { height: 200px; } }
    </style>
</head>
<body>
    <div class="container">
        <h1>Teorema da Entropia Democrática</h1>
        <div class="license-initial">
            <p>© Lara Raquel Rodrigues Branco Arsénio, 2025. Licenciado sob <a href="https://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a>. Inclui Chart.js (<a href="https://github.com/chartjs/Chart.js/blob/master/LICENSE">MIT License</a>).</p>
        </div>
        
        <h2>Resumo</h2>
        <p>A democracia em larga escala é um sistema estruturalmente limitado que converge para resultados medíocres, devido à qualidade do voto coletivo, aqui quantificada pela variável Qualidade de Vida (Q), que traduz, de forma abstrata e independente de contextos regionais (Europa, Centro, Desenvolvimento), o objetivo de melhoria global da qualidade de vida. A pressão por coesão grupal conduz as decisões coletivas ao mínimo denominador comum (MDC), comprometendo Q, que estabiliza em valores medianos (~50 numa escala de 0 a 100). Quanto maior a escala (n), isto é, o número de votantes, mais restritivos são os resultados, com entropia absoluta (estabilização de Q com desvio SD(Q) ≤ 1) a partir de um ponto crítico identificado dinamicamente. Simulações em JavaScript validam esta disfuncionalidade em populações numerosas, como a União Europeia (n = 500M). A disfuncionalidade é agravada por vulnerabilidades sistémicas, como intrusão, corrupção, governos sombra e lobbies desregulados. Este teorema demonstra a impossibilidade matemática de alcançar resultados ótimos em larga escala, sugerindo a necessidade de reduzir a escala ou adotar sistemas alternativos.</p>
        
        <h2>Definições</h2>
        <p><strong>Qualidade de Vida (Q):</strong> Índice em [0, 100], inspirado no Índice de Desenvolvimento Humano (IDH), abrangendo saúde, educação e rendimento. Exemplo: Q = 100 representa condições ideais; Q = 0 indica ausência de qualidade.</p>
        <p><strong>Mínimo Denominador Comum (MDC):</strong> Decisão coletiva, modelada como a mediana das preferências individuais (p* = median(p_i, ..., p_n)), onde p_i ∈ [0, 1] representa a preferência de cada votante. É a escolha mais consensual, mas subótima.</p>
        <p><strong>Entropia:</strong> Estagnação de Q em valores medianos (~50), com perda de potencial para decisões que maximizem Q > 60.</p>
        <p><strong>Escala (n):</strong> Número de votantes. O ponto de ruptura ocorre onde a variabilidade de Q é insignificante.</p>
        
        <h2>Enunciado</h2>
        <p>Nos sistemas democráticos majoritários, a pressão por coesão faz com que as decisões coletivas convirjam para o mínimo denominador comum, subotimizando Q. Para grandes valores de n, Q estabiliza em ~50, com entropia absoluta onde a variabilidade de Q é insignificante. Esta limitação estrutural torna matematicamente impossível alcançar decisões que otimizem Q.</p>
        
        <h2>Fórmulas Matemáticas</h2>
        <div class="formula">
            Fórmula da Entropia Democrática:<br>
           <p>
        Q<sub>n</sub> = 100 &cdot; med(p<sub>1</sub>, p<sub>2</sub>, ..., p<sub>n</sub>), 
        &sigma;(Q<sub>n</sub>) = 50 / (f(&mu;) &radic;n)
           </p>
        </div>
        
        <h2>Modelo</h2>
        <h3>Hipóteses</h3>
        <p>Preferências individuais p_i ∈ [0, 1] seguem uma distribuição normal (média 0.5, desvio padrão 0.2).<br>
        Decisão coletiva: p* = median(p_i, ..., p_n).<br>
        Qualidade de vida: Q = p* × 100.<br>
        Entropia absoluta: SD(Q) ≤ 1, ou seja, Q ∈ [48, 52] em 95% do tempo.</p>

        <!-- ===== Justificação Teórica da Variância e Ponto Crítico ===== -->

  <h3>Variância e Ponto Crítico</h3>
  <p>
    No contexto do Teorema da Entropia Democrática (TED), a qualidade de vida coletiva Q<sub>n</sub> é modelada como a mediana das preferências individuais p<sub>i</sub>, escalada por 100:
  </p>
  <div class="formula">
    <p>Q<sub>n</sub> = 100 &times; med(p<sub>1</sub>, p<sub>2</sub>, &hellip;, p<sub>n</sub>)</p>
  </div>
  <p>
    A variabilidade de Q<sub>n</sub> é quantificada pelo desvio padrão &sigma;(Q<sub>n</sub>), que, sob condições ideais, segue a relação:
  </p>
  <div class="formula">
    <p>&sigma;(Q<sub>n</sub>) = 50 / ( f(&mu;) &times; &radic;n )</p>
  </div>
  <p>
    Onde f(&mu;) é uma função que depende da distribuição das preferências individuais &mu;, e n é o número de participantes.
  </p>
  <p>
    Para que a variabilidade de Q<sub>n</sub> seja considerada insignificante, estabelecemos o critério &sigma;(Q<sub>n</sub>) &le; 1. Aplicando esse critério à fórmula acima, obtemos o ponto crítico n<sub>crit</sub>:
  </p>
  <div class="formula">
    <p>n<sub>crit</sub> &ge; (50 / f(&mu;))&sup2;</p>
  </div>
  <p>
    Este ponto crítico indica o número mínimo de participantes necessário para que a variabilidade de Q<sub>n</sub> se torne suficientemente pequena, resultando em entropia absoluta.
  </p>
  <p>
    Por exemplo, para uma distribuição uniforme das preferências individuais (f(&mu;) &asymp; 0.8), temos:
  </p>
  <div class="formula">
    <p>n<sub>crit</sub> &asymp; (50 / 0.8)&sup2; &asymp; 3906</p>
  </div>
  <p>
    Este valor sugere que, em populações com mais de 3900 participantes, a variabilidade de Q<sub>n</sub> se torna tão reduzida que qualquer decisão coletiva converge inevitavelmente para a mediana, caracterizando a entropia absoluta.
  </p>
<!-- ==== /Justificação Teórica da Variância e Ponto Crítico ==== -->

        <h2>Simulação</h2>
        
 
<canvas id="entropyComparisonChart"></canvas>
<p id="currentN"></p>

<h2>Cálculo da Variabilidade e Ponto Crítico</h2>

    <p><strong>Fórmula da variância:</strong></p>
   
        <div class="formula"><p>σ(Q<sub>n</sub>) = 50 / ( f(μ) · √n )</p>
        </div>
    <p><strong>Condição para entropia absoluta:</strong></p>
   <div class="formula"> <p>σ(Q<sub>n</sub>) ≤ 1</p>
   </div>
    <p><strong>Derivação do ponto crítico:</strong></p>
   <div class="formula"> <p>
        √n ≥ 50 / f(μ)<br>
        N<sub>crit</sub> ≥ (50 / f(μ))²
    </p>
   </div>
    <p><strong>Exemplo numérico (f(μ) ≈ 0.8):</strong></p>
    <ul>
        <li>N<sub>crit</sub> ≈ (50 / 0.8)² = 3906</li>
    </ul>

        <!-- ===== Justificação Teórica da Variância e Ponto Crítico ===== -->
  <section id="justificacao-variancia">
    <h3>Justificação Teórica da Variância e Ponto Crítico</h3>
    <p>
      No contexto do Teorema da Entropia Democrática (TED), a qualidade de vida coletiva Q<sub>n</sub> é modelada como a mediana das preferências individuais p<sub>i</sub>, escalada por 100:
    </p>
    <div class="formula">
      <p>Q<sub>n</sub> = 100 &times; med(p<sub>1</sub>, p<sub>2</sub>, &hellip;, p<sub>n</sub>)</p>
    </div>
    <p>
      A variabilidade de Q<sub>n</sub> é quantificada pelo desvio padrão &sigma;(Q<sub>n</sub>), que, sob condições ideais, segue a relação:
    </p>
    <div class="formula">
      <p>&sigma;(Q<sub>n</sub>) = 50 / ( f(&mu;) &times; &radic;n )</p>
    </div>
    <p>
      Para que a variabilidade de Q<sub>n</sub> seja considerada insignificante, estabelecemos o critério &sigma;(Q<sub>n</sub>) &le; 1. Aplicando esse critério à fórmula acima, obtemos o ponto crítico n<sub>crit</sub>:
    </p>
    <div class="formula">
      <p>n<sub>crit</sub> &ge; (50 / f(&mu;))&sup2;</p>
    </div>
    <p>
      Este valor sugere que, em populações com mais de 3900 participantes, a variabilidade de Q<sub>n</sub> se torna tão reduzida que qualquer decisão coletiva converge inevitavelmente para a mediana.
    </p>

    <!-- Gráfico de simulação da variabilidade -->
    <h4>Simulação da Variabilidade de Q<sub>n</sub> com n</h4>
    <canvas id="variabilidadeChart" width="600" height="300"></canvas>

  </section>
   <!-- ===== TED — f(mu) e n_crit (versão resumida) ===== -->
<section>

  <h2 class="fmu-title">Função f(&mu;) e Ponto Crítico n<sub>crit</sub> (Resumo)</h2>

  <div class="fmu-formula">
    <p style="margin:0">
      <strong>Definição.</strong>
      Seja &mu; a distribuição das preferências individuais p<sub>i</sub> &in; [0,1].
      Define-se f(&mu;) como a função que devolve o menor n tal que a variabilidade de Q<sub>n</sub> é entropizada:
      <br>
      n<sub>crit</sub> = f(&mu;),&nbsp; onde &nbsp; &sigma;(Q<sub>n</sub>) &le; 1 &nbsp; para todo n &ge; n<sub>crit</sub>.
      <br>
      Com &nbsp; Q<sub>n</sub> = 100 &middot; med(p<sub>1</sub>,...,p<sub>n</sub>) &nbsp; e, no caso teórico base, &nbsp; 
      &sigma;(Q<sub>n</sub>) = 50 / ( f(&mu;) &radic;n ).
    </p>
  </div>

  <p class="fmu-par"><strong>Intuição.</strong> Quanto mais a distribuição &mu; for simétrica e concentrada em torno de 0,5, mais cedo ocorre entropia (n<sub>crit</sub> pequeno). Quanto mais dispersa, multi-modal ou enviesada, maior o n<sub>crit</sub>.</p>

  <p class="fmu-par"><strong>Exemplos resumidos.</strong></p>
  <ul class="fmu-list">
    <li><em>Uniforme [0,1]</em>: mediana converge rapidamente para 0,5 &rarr; n<sub>crit</sub> pequeno (dezenas).</li>
    <li><em>Bimodal (picos em 0 e 1)</em>: população polarizada &rarr; n<sub>crit</sub> grande (centenas/milhares).</li>
    <li><em>Normal centrada em 0,5 (desvio pequeno)</em>: forte concentração no centro &rarr; n<sub>crit</sub> muito pequeno (&lt; dezenas).</li>
    <li><em>Enviesada (e.g., &mu; &asymp; 0,7)</em>: tendência clara &rarr; entropia ocorre, mas com n<sub>crit</sub> maior (centenas).</li>
  </ul>

  <p class="fmu-par"><strong>Nota.</strong> A forma fechada de f(&mu;) depende de &mu;. Na prática, estima-se n<sub>crit</sub> por simulação ou por limites assintóticos da variância da mediana sob a distribuição assumida.</p>
</section>
<!-- ==== /TED — f(mu) e n_crit (resumo) ==== -->

    <p><strong>Comparação de σ(Q<sub>n</sub>) para diferentes n:</strong></p>
    <table border="1" cellpadding="4" cellspacing="0">
        <thead>
            <tr>
                <th>n</th>
                <th>σ(Q<sub>n</sub>)</th>
                <th>Observação</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>100</td>
                <td>50 / (0.8·√100) ≈ 6.25</td>
                <td>Muito acima do critério</td>
            </tr>
            <tr>
                <td>1000</td>
                <td>50 / (0.8·√1000) ≈ 0.81</td>
                <td>Abaixo do critério → entropia praticamente garantida</td>
            </tr>
            <tr>
                <td>5000</td>
                <td>50 / (0.8·√5000) ≈ 0.44</td>
                <td>Abaixo do critério → entropia praticamente garantida</td>
            </tr>
            <tr>
                <td>20000</td>
                <td>50 / (0.8·√20000) ≈ 0.14</td>
                <td>Entropia absoluta confirmada</td>
            </tr>
            <tr>
                <td>100000</td>
                <td>50 / (0.8·√100000) ≈ 0.056</td>
                <td>Entropia total, praticamente sem variabilidade</td>
            </tr>
        </tbody>
    </table>
</div><br/>
<p><strong>Resumo do efeito de entropia:</strong> 
Em populações acima do ponto crítico N<sub>crit</sub>, a variabilidade de Q<sub>n</sub> torna-se tão reduzida que qualquer decisão coletiva converge inevitavelmente para a mediana das preferências individuais. 
Consequentemente, candidatos ou propostas extraordinárias não conseguem influenciar significativamente o resultado, consolidando o mínimo denominador comum como fator determinante da escolha coletiva.</p>
Este problema Matemático levanta um problema humano real. Como integrar com igual peso um grande número de votos sem cair num ciclo entropia. 
<br/>

    <section id="simulacoes-com-formulas">
    <h2>Simulações, Fórmulas e Interpretação</h2>
    
    <p>
        As simulações do Teorema da Entropia Democrática mostram que, à medida que o número de participantes aumenta, 
        a mediana das preferências individuais tende a estabilizar. Isto confirma que grandes grupos convergem para 
        um ponto de equilíbrio subótimo — o mínimo denominador comum das preferências.
    </p>

    <div id="result" style="white-space: pre-line; font-family: monospace; background:#f0f0f0; padding:10px; border:1px solid #ccc; max-height:400px; overflow-y:auto;">
        <!-- Resultados serão preenchidos aqui pelo JS -->
    </div>

    <h3>Fórmulas de Cálculo</h3>
    <div style="background:#fafafa; padding:10px; border:1px solid #ddd; margin-bottom:10px;">
        <strong>Mediana:</strong> <br>
        Q = 100 × mediana(p<sub>i</sub>)<br>
        <em>Onde p<sub>i</sub> são as preferências individuais geradas aleatoriamente.</em>
    </div>

    <div style="background:#fafafa; padding:10px; border:1px solid #ddd; margin-bottom:10px;">
        <strong>Desvio Padrão:</strong> <br>
        σ = sqrt( (∑ (p<sub>i</sub> - μ)²) / n ) <br>
        μ = média(p<sub>i</sub>) <br>
        <em>Permite medir a dispersão das preferências individuais em torno da mediana.</em>
    </div>

    <h3>Tabela Estatística das Simulações</h3>
    <table style="border-collapse: collapse; width:100%;">
        <thead>
            <tr>
                <th>Participantes (n)</th>
                <th>Mediana (Q)</th>
                <th>Desvio Padrão das Preferências</th>
            </tr>
        </thead>
        <tbody id="table-body">
            <!-- Linhas preenchidas pelo JS -->
        </tbody>
    </table>

    <p>
        O cálculo subjacente consiste em gerar aleatoriamente preferências individuais e calcular a mediana Q, 
        que representa a qualidade coletiva. Observa-se que a agregação de preferências produz um efeito de “achatamento” 
        dos extremos, típico de sistemas com entropia elevada.
    </p>

    <script>
        function gerarPreferencias(n) {
            let prefs = [];
            for (let i = 0; i < n; i++) {
                prefs.push(Math.random());
            }
            return prefs;
        }

        function mediana(arr) {
            let sorted = arr.slice().sort((a,b)=>a-b);
            let mid = Math.floor(sorted.length/2);
            return (arr.length % 2 === 0) ? (sorted[mid-1]+sorted[mid])/2 : sorted[mid];
        }

        function desvioPadrao(arr) {
            let media = arr.reduce((a,b)=>a+b,0)/arr.length;
            let variancia = arr.reduce((a,b)=>a+Math.pow(b-media,2),0)/arr.length;
            return Math.sqrt(variancia);
        }

        function simularTED(n) {
            let prefs = gerarPreferencias(n);
            let Q = mediana(prefs)*100;
            let dp = desvioPadrao(prefs);
            let texto = `Número de participantes: ${n}<br>Mediana (Q): ${Q.toFixed(2)}<br>Desvio padrão: ${dp.toFixed(2)}<br>----------------------------<br>`;
            return {texto, Q, dp};
        }

        let simulacoes = [5,50,500,5000,100000];
        let resultDiv = document.getElementById('result');
        let tbody = document.getElementById('table-body');

        simulacoes.forEach((n,i)=>{
            setTimeout(()=>{
                let res = simularTED(n);
                resultDiv.innerHTML += res.texto;

                let tr = document.createElement('tr');
                tr.innerHTML = `<td>${n}</td><td>${res.Q.toFixed(2)}</td><td>${res.dp.toFixed(2)}</td>`;
                tbody.appendChild(tr);
            }, i*500);
        });
    </script>
</section>

    
    <!-- ===================== TED — Aplicações em IA e Machine Learning ===================== -->

    
    <section id="aplicacoes-ia-ml" class="ted-ia-section">

  <h2 class="ted-ia-title">Aplicações em Inteligência Artificial e Machine Learning</h2>

  <p class="ted-ia-paragraph">
    A formulação do Teorema da Entropia Democrática (TED), originalmente concebida para explicar limitações estruturais na decisão coletiva em sistemas democráticos de larga escala, pode ser extrapolada para sistemas de aprendizagem automática (*machine learning*) e ciência de dados em grande escala.
  </p>

  <p class="ted-ia-paragraph">
    Nestes contextos, as entradas (inputs) de um modelo podem ser entendidas como análogas às preferências individuais (p<sub>i</sub>) no TED. Quando os dados são essencialmente <strong>qualitativos ou subjetivos</strong> (por exemplo, avaliações humanas, classificações, votos binários, preferências de utilizadores), o aumento da escala tende a produzir um efeito de <strong>medianização</strong>, isto é, convergência para padrões médios e consensuais em detrimento de extremos informativos.
  </p>

  <div class="ted-ia-formula">
    <p style="margin:0;">
      <strong>Analogia formal:</strong><br>
      Q<sub>n</sub> = 100 &middot; med(p<sub>1</sub>, p<sub>2</sub>, ..., p<sub>n</sub>) &rarr; Em IA/ML, a saída agregada de muitos inputs qualitativos tende ao valor mediano.<br>
      Entropia (SD(Q) &le; 1) &rarr; Redução da variabilidade informativa &amp; respostas medianas em larga escala.
    </p>
  </div>

  <p class="ted-ia-paragraph">
    Tal como em democracias de grande escala, onde <em>Q</em> converge para ~50, em sistemas de IA alimentados por milhões de inputs qualitativos, a diversidade efetiva degrada-se, resultando em <strong>respostas medianas e perda de nuance</strong>. A entropia manifesta-se como <strong>redução da qualidade dos dados e da capacidade preditiva</strong> com o crescimento indiscriminado da escala, mesmo que o volume total aumente.
  </p>

  <p class="ted-ia-paragraph">
    <strong>Hipótese derivada do TED:</strong> Em sistemas de *machine learning* baseados em inputs qualitativos massivos, a entropia democrática conduz inevitavelmente à degradação da qualidade média da resposta, impondo um limite estrutural ao ganho marginal obtido pelo aumento da escala.
  </p>

  <div class="ted-ia-highlight">
    <p class="ted-ia-paragraph" style="margin:0;">
      Aplicado a sistemas de IA e Machine Learning que dependem de inputs qualitativos (feedback humano, rotulagem subjetiva, preferências coletivas), o TED sugere que a expansão indiscriminada da base de dados não implica necessariamente melhor desempenho: pelo contrário, conduz à convergência para resultados medianos, degradando a qualidade, a capacidade de resposta e a precisão do sistema.
    </p>
  </div>

  <p class="ted-ia-paragraph ted-ia-note">
    Mitigações sugeridas pelo TED: segmentação/clusterização prévia (mini-círculos/mini-conjuntos), ponderação qualitativa, amostragem estratificada e mecanismos de preservação de diversidade informativa antes da agregação global.
  </p>

  <h3 class="ted-ia-subtitle">Quadro-resumo: Democracia vs. IA/ML</h3>
  <table class="ted-ia-table">
    <thead>
      <tr>
        <th>Dimensão</th>
        <th>Democracia (TED)</th>
        <th>IA/ML (Analogia)</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>Unidade básica</td>
        <td>Preferência individual p<sub>i</sub> &in; [0,1]</td>
        <td>Input qualitativo (rótulo, avaliação, voto binário, preferência)</td>
      </tr>
      <tr>
        <td>Agregador</td>
        <td>Mediana: p* = med(p<sub>1</sub>, ..., p<sub>n</sub>)</td>
        <td>Agregação de sinais (muitas vezes média/mediana; em feedback humano, maioria/consenso)</td>
      </tr>
      <tr>
        <td>Métrica de saída</td>
        <td>Q<sub>n</sub> = 100 &middot; p*</td>
        <td>Métrica de desempenho/qualidade predita (ex.: score, loss inversa, utilidade)</td>
      </tr>
      <tr>
        <td>Fenómeno de escala</td>
        <td>Convergência para ~50 com aumento de n</td>
        <td>Convergência para respostas medianas com grande volume de inputs qualitativos</td>
      </tr>
      <tr>
        <td>Entropia</td>
        <td>SD(Q) &le; 1 (variabilidade insignificante)</td>
        <td>Baixa variância informativa; perda de nuance; “medianização”</td>
      </tr>
      <tr>
        <td>Ponto crítico</td>
        <td>N<sub>crit</sub>: menor n com SD(Q) &le; 1</td>
        <td>N<sub>crit</sub> (analógico): escala a partir da qual a diversidade efetiva decai significativamente</td>
      </tr>
      <tr>
        <td>Risco principal</td>
        <td>Mínimo denominador comum (MDC) &rarr; subótimo coletivo</td>
        <td>Overfitting ao consenso/ruído; respostas medianas; perda de precisão contextual</td>
      </tr>
      <tr>
        <td>Mitigação</td>
        <td>Reduzir escala; subestruturas (mini-círculos); mecanismos deliberativos</td>
        <td>Curadoria/ponderação de dados; clusters; amostragem estratificada; preservação de diversidade</td>
      </tr>
    </tbody>
  </table>
</section>
<!-- =================== /TED — Aplicações em IA e Machine Learning ==================== -->

<div class="license">
            <p>© Lara Raquel Rodrigues Branco Arsénio (CC BY-SA 4.0)</p>
</div>
<script>
document.addEventListener('DOMContentLoaded', function(){

    const container = document.body;

    // Criar canvas se não existir
    let canvas = document.getElementById('entropyComparisonChart');
    if(!canvas){
        canvas = document.createElement('canvas');
        canvas.id = 'entropyComparisonChart';
        canvas.style.maxWidth = '900px';
        canvas.style.height = '450px';
        container.appendChild(canvas);
    }

    const ctx = canvas.getContext('2d');

    // Funções matemáticas
    function randomNormal(mean, stdDev){
        const u1 = Math.random();
        const u2 = Math.random();
        const z = Math.sqrt(-2*Math.log(u1)) * Math.cos(2*Math.PI*u2);
        return mean + stdDev*z;
    }

    function median(values){
        values.sort((a,b)=>a-b);
        const half = Math.floor(values.length/2);
        return values.length%2 ? values[half] : (values[half-1]+values[half])/2;
    }

    function simulateQn(n, sims){
        const results = [];
        for(let s=0; s<sims; s++){
            const props = [];
            for(let j=0; j<n; j++){
                let pi = randomNormal(0.5,0.2);
                pi = Math.min(Math.max(pi,0),1);
                props.push(pi);
            }
            results.push(100*median(props));
        }
        return results;
    }

    // Parâmetros
    const sampleSizes = [100, 5000, 20000, 100000]; // diferentes n
    const simsPerN = 100; // número de replicações
    const colors = ['blue','green','orange','red'];

    const chartData = {
        labels: Array.from({length:simsPerN}, (_,i)=>i+1),
        datasets: []
    };

    sampleSizes.forEach((n, idx) => {
        const qValues = simulateQn(n,simsPerN);
        chartData.datasets.push({
            label: `n = ${n}`,
            data: qValues,
            borderColor: colors[idx],
            fill:false,
            tension:0.2,
            pointRadius:2
        });
    });

    // Linha horizontal ponto de entropia (50)
    chartData.datasets.push({
        label: 'Ponto de entropia (50)',
        data: Array(simsPerN).fill(50),
        borderColor: 'black',
        borderDash:[5,5],
        fill:false,
        pointRadius:0
    });

    // Inicializar gráfico
    new Chart(ctx,{
        type:'line',
        data: chartData,
        options:{
            responsive:true,
            plugins:{
                tooltip:{enabled:true, mode:'index', intersect:false},
                legend:{display:true, position:'top'}
            },
            scales:{
                x:{ title:{display:true,text:'Replicação'} },
                y:{ title:{display:true,text:'Q_n'}, min:0, max:100 }
            }
        }
    });


});
// Dados da simulação
    const nValues = Array.from({length: 20}, (_, i) => (i + 1) * 200);
    const sigmaValues = nValues.map(n => 50 / (0.8 * Math.sqrt(n)));

    const ctx = document.getElementById('variabilidadeChart').getContext('2d');
    const variabilidadeChart = new Chart(ctx, {
        type: 'line',
        data: {
            labels: nValues,
            datasets: [{
                label: '&sigma;(Q_n)',
                data: sigmaValues,
                borderColor: 'rgba(75, 192, 192, 1)',
                backgroundColor: 'rgba(75, 192, 192, 0.2)',
                borderWidth: 2,
                fill: true,
                tension: 0.4
            }]
        },
        options: {
            responsive: true,
            plugins: {
                legend: {
                    display: true,
                    position: 'top'
                },
                title: {
                    display: false
                }
            },
            scales: {
                x: {
                    title: {
                        display: true,
                        text: 'Número de Participantes (n)'
                    }
                },
                y: {
                    title: {
                        display: true,
                        text: 'Desvio Padrão &sigma;(Q_n)'
                    },
                    beginAtZero: true
                }
            }
        }
    });
  
</script>


</body>
  </html>
